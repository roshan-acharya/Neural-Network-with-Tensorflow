{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958b5883",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#using sk;earn for PCA\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "#using sk;earn for PCA\n",
    "import pandas as pd\n",
    "import numpy as np # type: ignore\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15ba514",
   "metadata": {},
   "source": [
    "#### Curse of Dimesionality\n",
    "#### Approaches for dimensionality reduction\n",
    "##### a) Projection\n",
    "##### b) Manifold Learning \n",
    "\n",
    "### Principal Component Analysis (PCA) Notes\n",
    "\n",
    "#### 1. Definition\n",
    "PCA is by far and most popular dimensionality reduction algorithm.First it identifies the hyperplane that lies closest to the data and then it projects the data into it.\n",
    "\n",
    "---\n",
    "\n",
    "#### 2. Things to consider :\n",
    "Preserving variance : High variance - Data preserved\n",
    "\n",
    "#### 3. Assumption :\n",
    "All the dataset is centered around origin\n",
    "\n",
    "#### Steps : \n",
    "\n",
    "#### A. Principal component:\n",
    "It also finds a second axis, orthogonal to the\n",
    "first one, that accounts for the largest amount of remaining variance.The unit vector that defines the ith axis is called the ith principal component (PC).\n",
    "\n",
    "#### B. Projecting Down to d Dimensions:\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Key Concepts\n",
    "\n",
    "- **Variance:**\n",
    "- **Covariance Matrix:**\n",
    "- **Eigenvalues and Eigenvectors:**\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381daf18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1846b50",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#using sk;earn for PCA\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m      5\u001b[39m data = np.array([\n\u001b[32m      6\u001b[39m     [\u001b[32m2.5\u001b[39m, \u001b[32m2.4\u001b[39m, \u001b[32m0.5\u001b[39m, \u001b[32m1.2\u001b[39m],\n\u001b[32m      7\u001b[39m     [\u001b[32m0.5\u001b[39m, \u001b[32m0.7\u001b[39m, \u001b[32m1.5\u001b[39m, \u001b[32m0.3\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     15\u001b[39m     [\u001b[32m1.1\u001b[39m, \u001b[32m0.9\u001b[39m, \u001b[32m1.3\u001b[39m, \u001b[32m0.6\u001b[39m]\n\u001b[32m     16\u001b[39m ])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "data = np.array([\n",
    "    [2.5, 2.4, 0.5, 1.2],\n",
    "    [0.5, 0.7, 1.5, 0.3],\n",
    "    [2.2, 2.9, 1.0, 1.5],\n",
    "    [1.9, 2.2, 0.9, 1.1],\n",
    "    [3.1, 3.0, 0.7, 1.8],\n",
    "    [2.3, 2.7, 0.6, 1.3],\n",
    "    [2.0, 1.6, 0.8, 1.0],\n",
    "    [1.0, 1.1, 1.2, 0.5],\n",
    "    [1.5, 1.6, 1.1, 0.9],\n",
    "    [1.1, 0.9, 1.3, 0.6]\n",
    "])\n",
    "\n",
    "X = pd.DataFrame(data, columns=[\"Feature1\", \"Feature2\", \"Feature3\", \"Feature4\"])\n",
    "\n",
    "PCA = PCA(n_components=2)\n",
    "X2D= PCA.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd8847",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
